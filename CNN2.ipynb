{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 49 165  88 ... 112   8 108]\n",
      "X_train: (25000, 3136) \n",
      "y_train: (25000,)\n",
      "X_val: (5000, 3136) \n",
      "y_val: (5000,)\n",
      "X_train tensor: tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0., 255.,   0.,  ...,   0.,   0.,   0.]], dtype=torch.float64) \n",
      "y_train tensor: tensor([ 49, 165,  88,  ..., 155,  52, 241], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Define a split between training and validation data (since we were not provided with a separate validation dataset.)\n",
    "# This split helps us calculate validation loss while experimenting with different trainign techniques. However, our final\n",
    "# submission should be trained on all samples from the data we are given!\n",
    "VAL_SPLIT = 25000\n",
    "\n",
    "# Batch size is used to partition the training data into distinct batches which are used to train the CNN.\n",
    "# Batch creation is facilitated by pytorch's DataLoader module :)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# The number of epochs used to train the CNN.\n",
    "EPOCHS = 10\n",
    "\n",
    "# The loss function used during training\n",
    "import torch.nn.functional as F\n",
    "LOSS_FN = F.cross_entropy\n",
    "\n",
    "# The learning rate used in the SGD that trains the CNN.\n",
    "SGD_LEARNING_RATE = 0.1\n",
    "\n",
    "# The momentum used in the SGD that trains the CNN.\n",
    "SGD_MOMENTUM = 0.9\n",
    "\n",
    "import Data # Project data is loaded here\n",
    "from torch.utils.data import DataLoader, TensorDataset # PyTorch DataLoader and TensorDatasets ;;;)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Partition labeled data into train and val datasets\n",
    "X_tmp = Data.labeled_images.reshape(-1, 3136)\n",
    "y_tmp = Data.integer_labels() # Binary to number/letter, then number/letter to unique integer represntation for each label (conv net is only happy with 1D numerical outputs)\n",
    "print(y_tmp)\n",
    "X_val, y_val = X_tmp[VAL_SPLIT:], y_tmp[VAL_SPLIT:]\n",
    "X_train, y_train = X_tmp[0:VAL_SPLIT], y_tmp[0:VAL_SPLIT]\n",
    "print(f'X_train: {X_train.shape} \\ny_train: {y_train.shape}')\n",
    "print(f'X_val: {X_val.shape} \\ny_val: {y_val.shape}')\n",
    "\n",
    "# Transform the labeled train and val datasets into pytorch Tensors\n",
    "X_train, y_train, X_val, y_val = map(\n",
    "    torch.tensor, (X_train, y_train, X_val, y_val)\n",
    ")\n",
    "\n",
    "print(f'X_train tensor: {X_train} \\ny_train tensor: {y_train}')\n",
    "\n",
    "# Initialize pytorch TensorDatasets\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Initialize pytorch DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # Shuffled to prevent correlation betwen batch ordering and model overfitting, see: https://pytorch.org/tutorials/beginner/nn_tutorial.html#add-validation\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Calculate the loss for a single training batch\n",
    "def calc_batch_loss(model, loss_func, xb, yb, opt=None):\n",
    "    \n",
    "    # FOR DEBUGGING: PLEASE REMOVE IF IT DOESN\"T WORK\n",
    "    xb = xb.float()\n",
    "    yb = yb.long()\n",
    "\n",
    "    # Make predictions and get loss\n",
    "    predictions = model(xb)\n",
    "    predictions = predictions.float()\n",
    "    loss = loss_func(predictions, yb)\n",
    "\n",
    "    # Backpropagate if an optimizer is specified\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Return loss and length\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "# Fit an arbitrary model\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            calc_batch_loss(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[calc_batch_loss(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)\n",
    "\n",
    "# Lambda PyTorch Module that can be inserted into a \"Sequential\"\n",
    "# object to perform arbitrary lambda operations on NN layers.\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LeNET5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.99194658203125\n",
      "1 4.66392582244873\n",
      "2 4.550736502838134\n",
      "3 4.492142049407959\n",
      "4 4.524450703430176\n",
      "5 4.334594984436035\n",
      "6 4.580164798736572\n",
      "7 4.434666747283935\n",
      "8 4.5520953269958495\n",
      "9 4.214161065673828\n",
      "10 4.245454286956787\n",
      "11 4.376397064971924\n",
      "12 4.321677252197266\n",
      "13 4.2004441650390625\n",
      "14 4.161697741699219\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, preprocess):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        self.preprocess = preprocess # Preprocess lambda\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential( # Convolve to extract image features\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential( # Fully connected layers to learn classification task on extracted features\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocess(x) # Preprocess batch\n",
    "        x = self.feature_extractor(x) # Extract features\n",
    "        x = torch.flatten(x, 1) # Flatten to single layer\n",
    "        logits = self.classifier(x) # Make prediction using fully-connected layers\n",
    "        probs = F.softmax(logits, dim=1) # Softmax to get probabilities\n",
    "        return logits#, probs\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    batch = batch.view(-1, 1, 56, 56) # Resize input to make it square\n",
    "    downsample = nn.AdaptiveAvgPool2d((32, 32)) # Define a downsampling function\n",
    "    batch = downsample(batch) # Downsample the batch's image data\n",
    "    return batch\n",
    "\n",
    "# Declare model\n",
    "model = LeNet5(n_classes=260, preprocess=preprocess_batch)\n",
    "opt = optim.SGD(model.parameters(), lr=0.9)\n",
    "fit(15, model, F.cross_entropy, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": [
    "torch.no_grad() # Disable gradient before prediction!\n",
    "\n",
    "predictions = model(X_val.float())\n",
    "print(f'Prediction tensor: {predictions.shape}')\n",
    "\n",
    "correct = 0\n",
    "for i in range(0, len(y_val)):\n",
    "    true_label = y_val[i]\n",
    "    softmax_arr = predictions[i].detach().numpy()[0]\n",
    "    prediction = np.argmax(softmax_arr)\n",
    "    correct += int(true_label == prediction)\n",
    "\n",
    "print(correct / len(y_val))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
