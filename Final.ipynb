{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data standardized\n",
      "{0: '0A', 1: '1A', 2: '2A', 3: '3A', 4: '4A', 5: '5A', 6: '6A', 7: '7A', 8: '8A', 9: '9A', 10: '0B', 11: '1B', 12: '2B', 13: '3B', 14: '4B', 15: '5B', 16: '6B', 17: '7B', 18: '8B', 19: '9B', 20: '0C', 21: '1C', 22: '2C', 23: '3C', 24: '4C', 25: '5C', 26: '6C', 27: '7C', 28: '8C', 29: '9C', 30: '0D', 31: '1D', 32: '2D', 33: '3D', 34: '4D', 35: '5D', 36: '6D', 37: '7D', 38: '8D', 39: '9D', 40: '0E', 41: '1E', 42: '2E', 43: '3E', 44: '4E', 45: '5E', 46: '6E', 47: '7E', 48: '8E', 49: '9E', 50: '0F', 51: '1F', 52: '2F', 53: '3F', 54: '4F', 55: '5F', 56: '6F', 57: '7F', 58: '8F', 59: '9F', 60: '0G', 61: '1G', 62: '2G', 63: '3G', 64: '4G', 65: '5G', 66: '6G', 67: '7G', 68: '8G', 69: '9G', 70: '0H', 71: '1H', 72: '2H', 73: '3H', 74: '4H', 75: '5H', 76: '6H', 77: '7H', 78: '8H', 79: '9H', 80: '0I', 81: '1I', 82: '2I', 83: '3I', 84: '4I', 85: '5I', 86: '6I', 87: '7I', 88: '8I', 89: '9I', 90: '0J', 91: '1J', 92: '2J', 93: '3J', 94: '4J', 95: '5J', 96: '6J', 97: '7J', 98: '8J', 99: '9J', 100: '0K', 101: '1K', 102: '2K', 103: '3K', 104: '4K', 105: '5K', 106: '6K', 107: '7K', 108: '8K', 109: '9K', 110: '0L', 111: '1L', 112: '2L', 113: '3L', 114: '4L', 115: '5L', 116: '6L', 117: '7L', 118: '8L', 119: '9L', 120: '0M', 121: '1M', 122: '2M', 123: '3M', 124: '4M', 125: '5M', 126: '6M', 127: '7M', 128: '8M', 129: '9M', 130: '0N', 131: '1N', 132: '2N', 133: '3N', 134: '4N', 135: '5N', 136: '6N', 137: '7N', 138: '8N', 139: '9N', 140: '0O', 141: '1O', 142: '2O', 143: '3O', 144: '4O', 145: '5O', 146: '6O', 147: '7O', 148: '8O', 149: '9O', 150: '0P', 151: '1P', 152: '2P', 153: '3P', 154: '4P', 155: '5P', 156: '6P', 157: '7P', 158: '8P', 159: '9P', 160: '0Q', 161: '1Q', 162: '2Q', 163: '3Q', 164: '4Q', 165: '5Q', 166: '6Q', 167: '7Q', 168: '8Q', 169: '9Q', 170: '0R', 171: '1R', 172: '2R', 173: '3R', 174: '4R', 175: '5R', 176: '6R', 177: '7R', 178: '8R', 179: '9R', 180: '0S', 181: '1S', 182: '2S', 183: '3S', 184: '4S', 185: '5S', 186: '6S', 187: '7S', 188: '8S', 189: '9S', 190: '0T', 191: '1T', 192: '2T', 193: '3T', 194: '4T', 195: '5T', 196: '6T', 197: '7T', 198: '8T', 199: '9T', 200: '0U', 201: '1U', 202: '2U', 203: '3U', 204: '4U', 205: '5U', 206: '6U', 207: '7U', 208: '8U', 209: '9U', 210: '0V', 211: '1V', 212: '2V', 213: '3V', 214: '4V', 215: '5V', 216: '6V', 217: '7V', 218: '8V', 219: '9V', 220: '0W', 221: '1W', 222: '2W', 223: '3W', 224: '4W', 225: '5W', 226: '6W', 227: '7W', 228: '8W', 229: '9W', 230: '0X', 231: '1X', 232: '2X', 233: '3X', 234: '4X', 235: '5X', 236: '6X', 237: '7X', 238: '8X', 239: '9X', 240: '0Y', 241: '1Y', 242: '2Y', 243: '3Y', 244: '4Y', 245: '5Y', 246: '6Y', 247: '7Y', 248: '8Y', 249: '9Y', 250: '0Z', 251: '1Z', 252: '2Z', 253: '3Z', 254: '4Z', 255: '5Z', 256: '6Z', 257: '7Z', 258: '8Z', 259: '9Z'}\n",
      "Data functions defined\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Number of inferred classes: 260\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import pickle\n",
    "with open(\"data/images_l.pkl\", 'rb') as f: labeled_images = pickle.load(f)\n",
    "with open(\"data/labels_l.pkl\", 'rb') as f: labels = pickle.load(f)\n",
    "with open(\"data/images_ul.pkl\", 'rb') as f: unlabeled_images = pickle.load(f)\n",
    "with open(\"data/images_test.pkl\", 'rb') as f: images_test = pickle.load(f)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "X_temp = np.append(labeled_images, unlabeled_images, axis=0)\n",
    "X_temp = np.append(X_temp, images_test, axis=0)\n",
    "mean = X_temp.mean()\n",
    "std = X_temp.std()\n",
    "\n",
    "del X_temp\n",
    "\n",
    "labeled_images = (labeled_images - mean) / std\n",
    "unlabeled_images = (unlabeled_images - mean) / std\n",
    "images_test = (images_test - mean) / std\n",
    "print(\"Data standardized\")\n",
    "\n",
    "label_dict = {}\n",
    "for letter in range(0, 26):\n",
    "    for number in range(0, 10):\n",
    "        label = str(number) + chr(65 + letter)\n",
    "        label_dict[letter*10 + number] = label\n",
    "print(label_dict)\n",
    "\n",
    "# Get the unique integer for a given label (used by the pytorch CNN architecture.)\n",
    "def label_to_int(label):\n",
    "    key_list = list(label_dict.keys())\n",
    "    value_list = list(label_dict.values())\n",
    "    key_index = value_list.index(label)\n",
    "    return key_list[key_index]\n",
    "\n",
    "# Get the label that corresponds with the given integer (must be between 0 and 259 inclusive.)\n",
    "def int_to_label(int):\n",
    "    return label_dict[int]\n",
    "\n",
    "# The dataset of labels, as unique integers (i.e., size == 30,000)\n",
    "def integer_labels():\n",
    "    return arrmap(label_to_int, arrmap(binary_to_nl, labels))\n",
    "\n",
    "# Shortcut function that maps a numpy array to a numpy array using the given lambda (fn.)\n",
    "# I didn't want to keep writing this, though there is probably a better way to implement it... :/\n",
    "def arrmap(fn, arr: np.array):\n",
    "    return np.array(list(map(fn, arr)))\n",
    "\n",
    "# Converts a single label into NUMBER/LETTER form.\n",
    "# e.g., '[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]' --> '9D'\n",
    "def binary_to_nl(binary_label):\n",
    "    num = None\n",
    "    letter = None\n",
    "    for i in range(0, len(binary_label)):\n",
    "        entry = binary_label[i]\n",
    "        if i <= 9 and entry == 1:\n",
    "            num = i\n",
    "        elif entry == 1:\n",
    "            letter = chr(64 - 9 + i)\n",
    "    return str(num) + letter\n",
    "\n",
    "# Converts back from a unique integer generated by the nl_to_int method into a number/letter label representation\n",
    "def int_to_nl(int_label):\n",
    "    num = 0\n",
    "    for i in range(0, 10):\n",
    "        if i * 100 <= int_label:\n",
    "            num = i\n",
    "    letter = None\n",
    "    for i in range(0, 26):\n",
    "        if (int_label - i) % 100 == 0:\n",
    "            letter = chr(65 + i)\n",
    "    return str(num) + letter\n",
    "\n",
    "print(\"Data functions defined\")\n",
    "\n",
    "# Keys are integer labels, values are binary labels (original label)\n",
    "binary_label_dict = {}\n",
    "print(labels)\n",
    "for binary_label in labels:\n",
    "    integer_label = label_to_int(binary_to_nl(binary_label))\n",
    "    binary_label_dict[integer_label] = binary_label\n",
    "print(f'Number of inferred classes: {len(binary_label_dict)}')\n",
    "\n",
    "# To parse the integer (0-259) --> Binary\n",
    "def parsePrediction(prediction):\n",
    "    binary_prediction = binary_label_dict[prediction]\n",
    "    acc = \"\"\n",
    "    for num in binary_prediction: \n",
    "        a = str(int(num))\n",
    "        acc += a\n",
    "    return acc\n",
    "\n",
    "# To create a file to write to for submission\n",
    "def parseForSubmission(predictions):    \n",
    "    f = open(\"./submission.csv\", \"w\")\n",
    "    f.write(f\"# Id,Category\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(f\"{i},{str(parsePrediction(predictions[i]))}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def get_dataloaders(X, y, validation_split_index):\n",
    "    '''Gets the training and validation dataloaders for a given dataset by using an index to split into training and validation.'''\n",
    "    # get partitioned training and validation numpy arrays from the dataset\n",
    "    x_train, x_val = data.split_train_and_val(X, validation_split_index)\n",
    "    y_train, y_val = data.split_train_and_val(y, validation_split_index)\n",
    "\n",
    "    # Make tensors out of the numpy arrays\n",
    "    x_train, y_train, x_val, y_val = map(torch.tensor, (x_train, y_train, x_val, y_val))\n",
    "\n",
    "    # Make TensorDatasets\n",
    "    from torch.utils.data import TensorDataset\n",
    "    train_ds = TensorDataset(x_train, y_train)\n",
    "    val_ds = TensorDataset(x_val, y_val)\n",
    "\n",
    "    # Make DataLoaders\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dl = DataLoader(train_ds, batch_size=32)\n",
    "    val_dl = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "    # return training and validation dataloaders :)\n",
    "    return train_dl, val_dl\n",
    "\n",
    "def downsample(batch, size=32):\n",
    "    '''Downsamples the data to fit model requirements. I couldn't find a way around this that worked effectively...'''\n",
    "    batch = batch.view(-1, 1, 56, 56) # Resize input to make it square\n",
    "    downsample = nn.AdaptiveAvgPool2d(size) # Define a downsampling function\n",
    "    batch = downsample(batch) # Downsample the batch's image data\n",
    "    return batch\n",
    "\n",
    "import assignment_data as data\n",
    "from model_training import training_loop\n",
    "def train_model(net, optimizer, criterion, epochs=2, device='cpu'):\n",
    "    '''Loads data and trains a network on the image classification task.'''\n",
    "    SPLIT_INDEX = 25000 # 25,000 training and 5,000 validation\n",
    "    train_dl, val_dl = get_dataloaders(data.labeled_images, data.integer_labels(), SPLIT_INDEX)\n",
    "    training_loop(net, criterion, optimizer, train_dl, val_dl, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_definitions import Net, LeNet5\n",
    "from torch.nn import functional as F\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = F.cross_entropy\n",
    "train_model(Net(), optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
